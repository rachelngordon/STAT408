model <- nnet::multinom(NObeyesdad ~ ., data=test, method = "multinom", maxit=100)
predictobesity = predict(model,datatest)
table(datatest$NObeyesdad, predictobesity)
mean(as.character(datatest$NObeyesdad) != as.character(predictobesity))
#WIP
#Ordinal Net Packages + Stuff
library("ordinalNet")
install.packages("ordinalNet")
#WIP
#Ordinal Net Packages + Stuff
library("ordinalNet")
library("ordinalgmifs")
install.packages("ordinalgmifs")
#WIP
#Ordinal Net Packages + Stuff
library("ordinalNet")
library("ordinalgmifs")
data("hccframe")
y <- hccframe$group
x <- as.matrix(subset(hccframe, select = -group))
y <- as.factor(obese_clean$NObeyesdad)
o_x
o_x2 <- obese_clean %>% mutate(across(Gender:MTRANS, as.numeric))
y <- as.factor(obese_clean$NObeyesdad)
o_x
x<- as.matrix(o_x2)
fit1 <- ordinalNet(x, y, family = "cumulative", link = "logit", parallelTerms = TRUE, nonparallelTerms = FALSE)
o_x2 <- obese_clean %>% mutate(across(Gender:MTRANS, as.numeric)) %>% select(!NObeyesdad)
y <- as.factor(obese_clean$NObeyesdad)
o_x2
x<- as.matrix(o_x2)
fit1 <- ordinalNet(x, y, family = "cumulative", link = "logit", parallelTerms = TRUE, nonparallelTerms = FALSE)
fit1 <- ordinalNet(x, y, family = "cumulative", link = "logit", parallelTerms = TRUE, nonparallelTerms = FALSE)
head(coef(fit1, matrix = TRUE))
as.numeric(o_x)
summary(fit1)
knitr::opts_chunk$set(echo = TRUE)
obesity <- read.csv('ObesityDataSet_raw_and_data_sinthetic.csv')
head(obesity)
is.na(obesity)
obesity$NObeyesdad <- as.factor(obesity$NObeyesdad)
levels(obesity$NObeyesdad)
library(tidyverse)
obese %>%
mutate(obesity_levels = as.factor(case_when(
grepl("Overweight_Level", NObeyesdad) ~ "Overweight",
grepl("Obesity_Type",NObeyesdad) ~ "Obese",
TRUE ~ as.character(NObeyesdad)
)
), MTRANS = as.factor(MTRANS), CALC = as.factor(CALC), family_history_with_overweight = as.factor(family_history_with_overweight), FAVC = as.factor(FAVC), CAEC = as.factor(CAEC), SMOKE = as.factor(SMOKE), SCC = as.factor(SCC), Gender = as.factor(Gender))
obesity <- obesity %>%
mutate(obesity_levels = ifelse(NObeyesdad == "Insufficient_Weight", "underweight", ifelse(NObeyesdad == "Normal_Weight", "normal", ifelse(NObeyesdad == "Overweight_Level_I" | NObeyesdad == "Overweight_Level_II", "overweight", "obese"))))
head(obesity)
levels(as.factor(obesity$obesity_levels))
typeof(obesity$obesity_levels)
#Dividing data into training and test set
#Random sampling
samplesize = 0.7*nrow(obesity)
set.seed(1010)
index = sample(seq_len(nrow(obesity)), size = samplesize)
#Creating training and test set
datatrain = obesity[index,]
datatest = obesity[-index,]
library(MASS)
## fit ordered logit model and store results 'm'
m <- polr(as.factor(obesity_levels) ~ as.factor(Gender) + Age + Height + Weight + as.factor(family_history_with_overweight) + as.factor(FAVC) + FCVC + NCP + as.factor(CAEC) + as.factor(SMOKE) + CH2O + as.factor(SCC) + FAF + TUE + as.factor(CALC) + as.factor(MTRANS), data = datatrain, Hess=TRUE)
## view a summary of the model
summary(m)
#Compute confusion table and misclassification error
predictobesity = predict(m,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
stepAIC(m)
# took out NCP, CH20, FAF
m_new <- polr(as.factor(obesity_levels) ~ as.factor(Gender) + Age + Height + Weight + as.factor(family_history_with_overweight) + as.factor(FAVC) + FCVC + as.factor(CAEC) + as.factor(SMOKE) + TUE + as.factor(CALC) + as.factor(MTRANS), data = datatrain, Hess=TRUE)
#Compute confusion table and misclassification error
predictobesity = predict(m,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
#Compute confusion table and misclassification error
predictobesity = predict(m_new,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
#Compute confusion table and misclassification error
predictobesity = predict(m_new,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
## fit ordered logit model and store results 'm'
m3 <- polr(as.factor(obesity_levels) ~ as.factor(Gender) + Age + as.factor(family_history_with_overweight) + as.factor(FAVC) + FCVC + NCP + as.factor(CAEC) + as.factor(SMOKE) + CH2O + as.factor(SCC) + FAF + TUE + as.factor(CALC) + as.factor(MTRANS), data = datatrain, Hess=TRUE)
## view a summary of the model
summary(m3)
#Compute confusion table and misclassification error
predictobesity = predict(m_new,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
#Compute confusion table and misclassification error
predictobesity = predict(m3,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
stepAIC(m3)
stepAIC(m3)
## fit ordered logit model and store results 'm'
# remove gnder, FCVC, CH20, CALC
m4 <- polr(as.factor(obesity_levels) ~ Age + as.factor(family_history_with_overweight) + as.factor(FAVC) + NCP + as.factor(CAEC) + as.factor(SMOKE) + CH2O + as.factor(SCC) + FAF + TUE + as.factor(MTRANS), data = datatrain, Hess=TRUE)
## view a summary of the model
summary(m4)
#Compute confusion table and misclassification error
predictobesity = predict(m4,datatest)
table(datatest$obesity_levels, predictobesity)
mean(as.character(datatest$obesity_levels) != as.character(predictobesity))
cor(obesity)
cor(as.numeric(obesity))
barplot(obesity$NObeyesdad)
barplot(obesity$weight)
plot(obesity$height, obesity$weight)
plot(obesity$Height, obesity$Weight)
plot(obesity$Weight, obesity$Height, main = "Height vs Weight")
plot(obesity$Weight**2, obesity$Height, main = "Height vs Weight")
plot(obesity$Weight, obesity$Height**2, main = "Height^2 vs Weight")
plot(obesity$Weight, obesity$Height**2, main = "Height^2 vs Weight", col = obesity$NObeyesdad)
plot(obesity$Weight, obesity$Height^2, main = "Height^2 vs Weight", col = obesity$NObeyesdad)
plot(obesity$Weight, I(obesity$Height^2), main = "Height^2 vs Weight", col = obesity$NObeyesdad)
knitr::opts_chunk$set(echo = TRUE)
library(datasets)
data(mtcars)
library(datasets)
data(mtcars)
mtcars
exp(1.25961)
exp(0.05504)
1 - (1/(1+exp(-(-33.60517  + 1.25961*20 + 0.05504*180))))
# random split the data into 80% training and 20% test
set.seed(2022)
index.train <- sample(1:dim(mtcars)[1], 0.8 * dim(mtcars)[1])
data.train <- mtcars[index.train,]
data.test <- mtcars[-index.train,]
# fit a logistic regression on training set
logistic.model2 <- glm(am~mpg+hp, data=data.train, family='binomial')
# predict on the test test, obtain the predicted P(Y=1)
p.pred <- predict(logistic.model2, data.test, type='response')
# transform to binary response
y.pred <- ifelse(p.pred>=0.5, 1, 0)
# calculate classification accuracy
y.truth <- data.test$am
acc.test <- mean(y.pred==y.truth)
acc.test
# confusion matrix
table(y.pred, y.truth)
# true positive
TP <- intersect(which(y.truth==1), which(y.pred==1))
# true negative
TN <- intersect(which(y.truth==0), which(y.pred==0))
# all positives
AP <- which(data.test$AHD==1)
# all negatives
AN <- which(data.test$AHD==0)
# predicted positives
PP <- which(y.pred==1)
# true postive rate
TPR <- length(TP) / length(AP)
TPR
# true negative rate
TNR <- length(TN) / length(AN)
TNR
# precision
prec <- length(TP) / length(PP)
prec
seatpos <- read.csv("seatpos.csv")
lmodel <- lm(hipcenter~., data = seatpos)
summary(lmodel)
cor(seatpos)
fat <- read.csv("fat.csv")
# remove every tenth observation for test set
fat %>% filter(row_number() != 1)
fat %>% filter(row_number()-1 %% 10 != 1)
fat <- read.csv("fat.csv")
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
fat <- fat %>% filter(row_number() != 1)
fat <- read.csv("fat.csv")
fat
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
fat <- fat %>% filter(row_number() != 1)
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
fat <- fat %>% filter(row_number() != 1)
fat
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
#fat <- fat %>% filter(row_number() != 1)
fat
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
fat <- fat %>% filter(row_number() != 1)
fat
# remove every tenth observation for test set
fat <- fat %>% filter(row_number()-1 %% 10 != 1)
#fat <- fat %>% filter(row_number() != 1)
fat
fat <- read.csv("fat.csv")
fat
fat <- read.csv("fat.csv")
fat
test_i <- seq(1, 252, by = 10)
test_i
fat <- read.csv("fat.csv")
fat
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i]
train
fat <- read.csv("fat.csv")
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i]
train
fat <- read.csv("fat.csv")
test_i <- seq(1, 252, by = 10)
fat[-test_i]
fat <- read.csv("fat.csv")
test_i <- seq(1, 252, by = 10)
fat[-test_i]
fat[test_i]
fat <- read.csv("fat.csv")
test_i <- seq(1, 252, by = 10)
fat[-test_i]
fat <- read.csv("fat.csv")
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i]
test <- fat[test_i]
fat <- read.csv("fat.csv")
fat
# divide training and testing sets
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i,]
test <- fat[test_i,]
train
# divide training and testing sets
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i,]
test <- fat[test_i,]
train
test
fat <- read.csv("fat.csv")
# divide training and testing sets
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i,]
test <- fat[test_i,]
# linear model
lm_fat <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearn + wrist, data = train)
# linear model
lm_fat <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data = train)
# make predictions
y.pred <- predict(lm_fat, newdata = test)
# calculate test MSE
y.test <- test$siri
MSE.test <- mean((y.test - y.pred)^2)
# root MSE
RMSE.test <- sqrt(MSE.test)
RMSE.test
stepAIC(lm_fat)
# new linear model
lm_new <- lm(siri ~ weight + adipos + free + chest + abdom + thigh + ankle + biceps + forearm, data = train)
# make predictions
y.pred <- predict(lm_new, newdata = test)
# calculate test MSE
y.test <- test$siri
MSE.test <- mean((y.test - y.pred)^2)
# root MSE
RMSE.test <- sqrt(MSE.test)
RMSE.test
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearn + wrist, train, lambda = seq(0, 5e-8, len=21))
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(testmeat[,-101])) %*% coef(rgmod)[8,]
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, trainmeat, lambda = seq(0, 5e-8, len=21))
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(testmeat[,-101])) %*% coef(rgmod)[8,]
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(test[,-101])) %*% coef(rgmod)[8,]
# Lasso
require(lars)
# lars function requires the matrix of predictors as its first argument,
# and the vector of response as its second argument
lmod <- lars(as.matrix(fat[,-4]),fat$siri)
install.packages("lars")
# Lasso
require(lars)
# lars function requires the matrix of predictors as its first argument,
# and the vector of response as its second argument
lmod <- lars(as.matrix(fat[,-4]),fat$siri)
plot(lmod)
# lasso for prediction
trainy <- train$siri
trainx <- as.matrix(train[,-101])
lassomod <- lars(trainx,trainy)
# select best penalty by cross-validation
set.seed(2022)
cvout <- cv.lars(trainx,trainy)
cvout$index[which.min(cvout$cv)]
# predict using best penalty
testx <- as.matrix(test[,-101])
yhat <- predict(lassomod,testx,s=0.0101,mode="fraction")
MSE <- mean((yhat$fit - test$siri)^2)
MSE
# Lasso
require(lars)
# lars function requires the matrix of predictors as its first argument,
# and the vector of response as its second argument
lmod <- lars(as.matrix(fat[,-4]),fat$siri)
plot(lmod)
# lasso for prediction
trainy <- train$siri
trainx <- as.matrix(train[,-101])
lassomod <- lars(trainx,trainy)
# select best penalty by cross-validation
set.seed(2022)
cvout <- cv.lars(trainx,trainy)
#cvout$index[which.min(cvout$cv)]
# predict using best penalty
testx <- as.matrix(test[,-101])
yhat <- predict(lassomod,testx,s=0.0101,mode="fraction")
MSE <- mean((yhat$fit - test$siri)^2)
# root MSE
RMSE.test <- sqrt(MSE)
RMSE.test
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(test[,-101])) %*% coef(rgmod)[8,]
# ridge regression
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, fat, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# regular linear model
modlm <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train)
yhat <- predict(modlm, test)
MSE <- mean((yhat - test$siri)^2)
MSE
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, trai, lambda = seq(0, 5e-8, len=21))
# ridge regression
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, fat, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# regular linear model
modlm <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train)
yhat <- predict(modlm, test)
MSE <- mean((yhat - test$siri)^2)
MSE
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(test[,-101])) %*% coef(rgmod)[8,]
fat <- read.csv("fat.csv")
# divide training and testing sets
test_i <- seq(1, 252, by = 10)
train <- fat[-test_i,]
test <- fat[test_i,]
# ridge regression
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, fat, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# regular linear model
modlm <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train)
yhat <- predict(modlm, test)
MSE <- mean((yhat - test$siri)^2)
MSE
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(1,as.matrix(test[,-101])) %*% coef(rgmod)[8,]
# ridge regression
require(MASS)
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, fat, lambda = seq(0, 5e-8, len=21))
matplot(rgmod$lambda, coef(rgmod), type="l", xlab=expression(lambda)
,ylab=expression(hat(beta)),col=1)
# regular linear model
modlm <- lm(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train)
yhat <- predict(modlm, test)
MSE <- mean((yhat - test$siri)^2)
MSE
# select best lamda by cross-validation
rgmod <- lm.ridge(siri ~ age + weight + height + adipos + free + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, train, lambda = seq(0, 5e-8, len=21))
rgmod$GCV
which.min(rgmod$GCV)
# predict using best lamda
yhat.ridge <- cbind(as.matrix(1),as.matrix(test[,-101])) %*% coef(rgmod)[8,]
library(babynames)
babynames
library(shiny)
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
titlePanel("Old Faithful Geyser Data"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("bins",
"Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
output$distPlot <- renderPlot({
# generate bins based on input$bins from ui.R
x    <- faithful[, 2]
bins <- seq(min(x), max(x), length.out = input$bins + 1)
# draw the histogram with the specified number of bins
hist(x, breaks = bins, col = 'darkgray', border = 'white',
xlab = 'Waiting time to next eruption (in mins)',
main = 'Histogram of waiting times')
})
}
# Run the application
shinyApp(ui = ui, server = server)
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
babynames %>%
select(sex == sex, year <= endYear & year >= startYear)
library(babynames)
babynames %>%
select(sex == sex, year <= endYear & year >= startYear)
babynames %>%
select(sex == "M", year <= 2000 & year >= 1970)
babynames %>%
select(sex == "M", year <= 2000 & year >= 1970)
library(babynames)
library(tidyverse)
babynames %>%
select(sex == "M")
babynames %>% select(sex == "M")
babynames %>% select(where(sex == "M"))
babynames %>% filter(sex == "M")
babynames %>% filter(sex == "M", year >= 1970)
babynames %>%
filter(sex == "M", year >= 1970 & year <= 2000)
babynames %>%
filter(sex == "M", year >= 1970 & year <= 2000) %>%
group_by(name) %>%
summarize(n = sum(n))
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
runApp('~/Top10BabyNames')
?arrange
library(babynames)
library(tidyverse)
output$table <- renderDataTable({
babynames %>%
filter(sex == input$sex, year >= input$startYear & year <= input$endYear) %>%
group_by(name) %>%
summarize(n = sum(n)) %>%
arrange()
})
babynames %>%
filter(sex == "M", year >= 1970 & year <= 2000) %>%
group_by(name) %>%
summarize(n = sum(n)) %>%
arrange()
runApp('~/Top10BabyNames')
