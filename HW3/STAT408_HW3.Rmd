---
title: "STAT408_HW3"
output:
  pdf_document: default
  html_document: default
date: "2022-10-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Question 1

(10 points) In this question, we will use the prostate dataset. Import this dataset and answer following questions.

```{r}
prostate <- read.csv("prostate.csv")
```

## part (a)

Compute a 95% CI for the parameter associated with age. Use the manual method.

```{r}
fit5 <- lm(lpsa ~ ., data = prostate)
summary(fit5)$coef[4,1] + c(-1,1) * qnorm(0.975) * summary(fit5)$coef[4,2]
```

## part (b)

Compute a 90% CI for the parameter associated with age. Use the manual method.

```{r}
summary(fit5)$coef[4,1] + c(-1,1) * qnorm(0.95) * summary(fit5)$coef[4,2]
```

## part (c)

Based on these two CIs, what can we expect the p-value of this parameter in t-test? Compare your conclusion with the p-value output by summary function.

Based on these two confidence intervals, we can expect that the p-value for the age parameter given by the t test will likely not be significant at a level of alpha = 0.05 because the first confidence interval includes zero. However, the second confidence interval does not include zero and includes only negative values, suggesting that the parameter associated with age may be slightly significant.

```{r}
summary(fit5)
```

These expectations were correct as the parameter for age is not significant at the level alpha = 0.05. However, it does appear to be significant at the level alpha = 0.1, because the p-value of 0.08229 is less than 0.1.


## part (d)

Conduct a permutation t-test for predictor age in this model.

```{r}
# orginal t test 
lm.model <- lm(lpsa~., data = prostate)
summary(lm.model)
summary(lm.model)$coef[4,]
T.original <- summary(lm.model)$coef[4,3]

# set random seed to keep result same
set.seed(123)

# empty vector to save each permutation T statistic
Ts <- c()

for(i in 1:1000){
  # linear regression on shuffled pregnant
  lm.model <- lm(lpsa~lcavol+lweight+sample(age)+lbph+svi+lcp+gleason+pgg45, data = prostate)
  # save permutation T statistic
  Ts[i] <-  summary(lm.model)$coef[4,3]
}

# Calculate the proportion of less than the original T statistics
mean(abs(Ts) > abs(T.original))
hist(Ts, breaks = 100)

```

The p-value for the permutation t-test for the predictor age in this model is 0.075, which means that age is not a significant predictor of lpsa at the level alpha = 0.05

## part (e)

Remove all the predictors not significant at the 5% level. Use anova function to conduct an F test to test this model against the original full model. Which model is preferred? Give your reason.

```{r}
fit6 <- lm(lpsa ~ lcavol + lweight + svi, data = prostate)
anova(fit6, fit5)
```

Based on the results of this F test, model 1 is preferred because the p-value of 0.2167 is greater than a significance level of alpha = 0.05, suggesting that the additional parameters in the full model are not significant or necessary.


# Question 2

(10 points) In this question, we will use the cheddar dataset. Import this dataset and answer following questions.

```{r}
cheddar <- read.csv('cheddar.csv')
```

## part (a)

Fit a regression model with taste as the response and the three chemical contents as predictors. Identify the predictors that are statistically significant at the 5% level.

```{r}
fit <- lm(taste ~ Acetic + H2S + Lactic, data = cheddar)
summary(fit)
```

H2S and Lactic are both significant predictors of taste at the level alpha = 0.05


## part (b)

Acetic and H2S are measured on a log scale. Fit a linear model where all three predictors are measured on their original scale. Identify the predictors that are statistically significant at the 5% level for this model. Hint: exponential function is exp().

```{r}
fit1 <- lm(taste ~ exp(Acetic) + exp(H2S) + Lactic, data = cheddar)
summary(fit1)
```

Only Lactic acid is a significant predictor of taste at the level alpha = 0.05 in this model.


## part (c)

Can we use an F-test to compare these two models? Which model provides a better fit to the data? Explain your reasoning for these two questions.

No, we cannot use an F-test to compare these two models because they are not a smaller model and a full model. In other words, there is not a model that contains all of the predictors in the other model plus some additional predictors. Therefore, we cannot use an F-test to compare two models where one model is not "nested" within the other model. 

However, we can conclude that the first model appears to be a better fit for the data because the adjusted $R^2$ value of 0.6116 is greater than 0.5264. Additionally, it has two predictors, H2S and Lactic, that are significant at the level alpha = 0.05.

## part (d)

If H2S is increased 0.01 for the model used in (a), what change in the taste would be expected?

```{r}
3.9118*0.01
```

If H2S is increased by 0.01 for the model used in part a, taste is expected to increase by 0.039118 units.


# Question 3

(10 points) In this question, we will use the teengamb dataset. Import this dataset and answer following questions.

```{r}
teengamb <- read.csv("teengamb.csv")
```

## part (a)

Fit a model with gamble as the response and the other variables as predictors. Which variables are statistically significant at the 5% level?

```{r}
fit2 <- lm(gamble ~ ., data = teengamb)
summary(fit2)
```

Sex and income are significant predictors of gambling at the level alpha = 0.05.

## part (b)

Check the meaning of each variable. Does the variable significance in (a) make sense? Give your reasoning.

Yes, it makes sense that men may be more likely to spend more on gambling than women and that those with a higher income would spend more on gambling because they have more money to spend. 

## part (c)

Fit a model with just income as a predictor and use an F-test to compare it to the full model.

```{r}
fit3 <- lm(gamble ~ income, data = teengamb)
anova(fit3, fit2)
```

Based on the results of this F test, the full model appears to be preferred because the p-value of 0.01177 is less than a significance level of alpha = 0.05, suggesting that the additional parameters in the full model are significant and thus that the full model is a better fit for the data.


# Question 4

(10 points) In this question, we will use the sat dataset. It was collected to study the relationship between expenditures on public education and test results. It contains the following variables

Expend: Current expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of dollars)
Ratio: Average pupil/teacher ratio in public elementary and secondary schools, Fall 1994
Salary: Estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of dollars)
Takers: Percentage of all eligible students taking the SAT, 1994-95 Verbal: Average verbal SAT score, 1994-95
Math: Average math SAT score, 1994-95
Total: Average total score on the SAT, 1994-95

Using the sat dataset, fit a linear model with the total SAT score as the response and expend, salary, ratio, and takers as predictors. Perform regression diagnostics on this model to answer the following questions. Display any plots that are relevant. Some questions may be subjective. Show the most valid judgment and give your seasons.

```{r}
sat <- read.csv("sat.csv")
fit4 <- lm(total ~ expend + salary + ratio + takers, data = sat)
```

## part (a)

Plot residual vs. fitted response to check the constant variance assumption for the errors.

```{r}
plot(fit4$fitted.values, fit4$residuals)
```

For the most part, the assumption of constant variance appears to be met based on this plot of the residuals versus the fitted values. There does appear to be an extremely slight megaphone shape in the graph but this seems to be due to a couple outliters (such as the point at the bottom of the graph between 1000 and 1050 on the x-axis) rather than the overall trend.

## part (b)

Use Q-Q plot to check the normality assumption. What is the shape of the error distribution?

```{r}
qqnorm(fit4$residuals)
qqline(fit4$residuals)
```

Based on the qq plot, the normality assumption does not appear to be met, as the qq plot appears to have short tails on either end that are not following the line.

## part (c)

Use studentized residuals to check the outliers. Set the cutoff of being “large” as the 5% critical value in t distribution. Note that we need to consider the two sides.

```{r}
# studentized residues
sr <- rstudent(fit4)
n <- dim(sat)[1]
df <- n - 4 - 1
# use 5% critical value as cutoff
which(abs(sr) > qt(0.975, df))
sum(abs(sr) > qt(0.975, df))
```

## part (d)

Using defbeta function to plot the change of parameter estimation and check influential points. Check influential points in terms of each parameter except the intercept.

```{r}
# influential observation
beta.change <- dfbeta(fit4)
# plot the change of expend parameter after removing each data point
plot(beta.change[,2])
# plot the change of salary parameter after removing each data point
plot(beta.change[,3])
# plot the change of ratio parameter after removing each data point
plot(beta.change[,4])
# plot the change of takers parameter after removing each data point
plot(beta.change[,5])
```


# Question 5

(10 points) The divusa dataset records the US divorce and social-economic variables in 77 years.

Year: the year from 1920-1996
Divorce: divorce per 1000 women aged 15 or more Unemployed: unemployment rate
Femlab: percent female participation in labor force aged 16+ Marriage: marriages per 1000 unmarried women aged 16+ Birth: births per 1000 women aged 15-44
Military: military personnel per 1000 population

Fit a model with divorce as the response and the other variables, except year, as predictors. Check for error correlation using three methods: plot residuals vs. years; plot residual (t+1) vs. residual (t); fit a linear model of residual (t+1) ~ residual (t). Give your insight and conclusion.

```{r}
divusa <- read.csv("divusa.csv")
fit7 <- lm(divorce ~ unemployed + femlab + marriage + birth + military, data = divusa)
# plot residuals vs years
plot(divusa$year, fit7$residuals)
# plot residual (t+1) vs. residual (t)
n <- dim(divusa)[1]
cor(tail(fit7$residuals, n-1), head(fit7$residuals, n-1))
plot(tail(fit7$residuals, n-1)~head(fit7$residuals, n-1))
# fit a linear model of residual (t+1) ~ residual (t)
fit8 <- lm(tail(fit7$residuals, n-1)~head(fit7$residuals, n-1))
summary(fit8)
```

The plot of residuals versus years does show a curved pattern, especially around 1980 where these is a distinct peak. The plot comparing the residuals of t and t+1 observations shows a somewhat linear pattern but the points are definitely more spaced out toward the right side of the graph. Additionally, the linear model of residual (t+1) ~ residual (t) has a p-value of <2e-16 for the predictor, which is significant at the level alpha = 0.05. Therefore, these results do show indications of error correlation for this model.